# NIPS 2017 Workshop: Deep Learning At Supercomputer Scale

Five years ago, it took more than a month to train a state-of-the-art image recognition model on the ImageNet dataset. Earlier this year, Facebook demonstrated that such a model could be trained in an hour. However, if we could parallelize this training problem across the world’s fastest supercomputers (~100 PFlops), it would be possible to train the same model in under a minute. This workshop is about closing that gap: how can we turn months into minutes and increase the productivity of machine learning researchers everywhere?

This one-day workshop will facilitate active debate and interaction across many different disciplines. The conversation will range from algorithms to infrastructure to silicon, with invited speakers from Cerebras, DeepMind, Facebook, Google, OpenAI, and other organizations. When should synchronous training be preferred over asynchronous training? Are large batch sizes the key to reach supercomputer scale, or is it possible to fully utilize a supercomputer at batch size one? How important is sparsity in enabling us to scale? Should sparsity patterns be structured or unstructured? To what extent do we expect to customize model architectures for particular problem domains, and to what extent can a “single model architecture” deliver state-of-the-art results across many different domains? How can new hardware architectures unlock even higher real-world training performance?

Our goal is bring people who are trying to answer any of these questions together in hopes that cross pollination will accelerate progress towards deep learning at true supercomputer scale.

# Confirmed Speakers

1. Priya Goyal - "ImageNet in 1 Hour" - Facebook Research
2. Timothy Lillicrap - "Scalable RL & AlphaGo" - DeepMind
3. Nitish Keskar - "Generalization Gap" - Salesforce Research
4. Scott Gray - "Small World Network Architectures" - OpenAI
5. Matthew Johnson - "KFAC and Second Order Optimizers" - Google Brain
6. Tim Salimans - "Evolutionary Strategies" - OpenAI
7. Elad Hoffer - "Closing the Generalization Gap" - Technion
8. Andy Hock - "Scaling with Small Batches" - Cerebras
9. Azalia Mirhoseini - "Learning Device Placement" - Google Brain
10. Gregory Diamos - "Scaling and Sparsity" - Baidu

# Important Dates

* 3 November 2017: Submissions due, 4:59pm PST
* 10 November 2017: Decisions announced
* 28 November 2017: Final copies due
* 9 December 2017: Workshop

# Participation

We welcome participation in the form of papers/posters and questions for the two panels that we will hold.  

For papers and posters please submit 2 page papers in NIPS format to `supercomputersfordl2017@gmail.com` with `NIPS Workshop Papers` in the subject line.  Accepted papers will be posted on the website and invited to display a poster at the workshop.

For panel questions please submit them to `supercomputersfordl2017@gmail.com` with `NIPS Workshop Questions` in the subject line.  The panels are TBD, but will be announced weeks before the conference.

# Registration Awards

We will offering NIPS workshop registrations to the four best papers submitted by students at academic instituions.  Please indicate in your submission if you would like to apply for this award.

# Contact Us

Feel free to reach us at `supercomputersfordl2017@gmail.com` with any questions you might have.

# Schedule
Coming soon...

# Organizers
* Erich Elsen, Google Brain
* Danijar Hafner, University College London
* Zak Stone, Google Brain
* Brennan Saeta, Google Brain
